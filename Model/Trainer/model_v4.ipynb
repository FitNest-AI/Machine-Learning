{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_estimator\\python\\estimator\\util.py:74: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\native_module.py:92: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\saved_model_module.py:40: The name tf.saved_model.constants.LEGACY_INIT_OP_KEY is deprecated. Please use tf.compat.v1.saved_model.constants.LEGACY_INIT_OP_KEY instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility Commands\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.11):\n",
    "\n",
    "  keypoints_all = []\n",
    "  keypoint_edges_all = []\n",
    "  edge_colors = []\n",
    "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "  for idx in range(num_instances):\n",
    "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "    kpts_absolute_xy = np.stack(\n",
    "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "        kpts_scores > keypoint_threshold, :]\n",
    "    keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "        keypoint_edges_all.append(line_seg)\n",
    "        edge_colors.append(color)\n",
    "  if keypoints_all:\n",
    "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "  else:\n",
    "    keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "  if keypoint_edges_all:\n",
    "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "  else:\n",
    "    edges_xy = np.zeros((0, 2, 2))\n",
    "  return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  # To remove the huge white borders\n",
    "  fig.tight_layout(pad=0)\n",
    "  ax.margins(0)\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_xticklabels([])\n",
    "  plt.axis('off')\n",
    "\n",
    "  im = ax.imshow(image)\n",
    "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "  ax.add_collection(line_segments)\n",
    "  # Turn off tick labels\n",
    "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "  (keypoint_locs, keypoint_edges,\n",
    "   edge_colors) = _keypoints_and_edges_for_display(\n",
    "       keypoints_with_scores, height, width)\n",
    "\n",
    "  line_segments.set_segments(keypoint_edges)\n",
    "  line_segments.set_color(edge_colors)\n",
    "  if keypoint_edges.shape[0]:\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "  if keypoint_locs.shape[0]:\n",
    "    scat.set_offsets(keypoint_locs)\n",
    "\n",
    "  if crop_region is not None:\n",
    "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "    rect = patches.Rectangle(\n",
    "        (xmin,ymin),rec_width,rec_height,\n",
    "        linewidth=1,edgecolor='b',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "  fig.canvas.draw()\n",
    "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "  image_from_plot = image_from_plot.reshape(\n",
    "      fig.canvas.get_width_height()[::-1] + (3,))\n",
    "  plt.close(fig)\n",
    "  if output_image_height is not None:\n",
    "    output_image_width = int(output_image_height / height * width)\n",
    "    image_from_plot = cv2.resize(\n",
    "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "         interpolation=cv2.INTER_CUBIC)\n",
    "  return image_from_plot\n",
    "\n",
    "# def to_gif(images, duration):\n",
    "#   \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n",
    "#   imageio.mimsave('./animation.gif', images, duration=duration)\n",
    "#   return embed.embed_file('./animation.gif')\n",
    "\n",
    "def progress(value, max=100):\n",
    "  return HTML(\"\"\"\n",
    "      <progress\n",
    "          value='{value}'\n",
    "          max='{max}',\n",
    "          style='width: 100%'\n",
    "      >\n",
    "          {value}\n",
    "      </progress>\n",
    "  \"\"\".format(value=value, max=max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonDetector:\n",
    "    def __init__(self):\n",
    "        model_path = \"https://www.kaggle.com/models/google/movenet/frameworks/TensorFlow2/variations/singlepose-thunder/versions/4\"\n",
    "        self.movenet = self.load_model(model_path)\n",
    "    \n",
    "    def load_model(self,model_path):\n",
    "        model = hub.load(model_path)\n",
    "        movenet = model.signatures['serving_default']\n",
    "        return movenet\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.movenet\n",
    "    \n",
    "    def detect(self,input_image,inference_count=3):\n",
    "        input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "        outputs = self.movenet(input_image)\n",
    "        keypoints_with_scores = outputs['output_0'].numpy()\n",
    "        return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor:\n",
    "    def process_image(self,image_path):\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.compat.v1.image.decode_jpeg(image)\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "        image = tf.cast(tf.image.resize_with_pad(image, 256, 256), dtype=tf.int32)\n",
    "        return image\n",
    "    \n",
    "    def display_image(self,image):\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        plt.grid(False)\n",
    "        plt.imshow(image[0])\n",
    "        plt.show()\n",
    "        \n",
    "    def display_image_with_keypoints(self,image,keypoints):\n",
    "        display_image = tf.cast(tf.image.resize_with_pad(\n",
    "            image, 1280, 1280), dtype=tf.int32)\n",
    "        output_overlay = draw_prediction_on_image(\n",
    "            np.squeeze(display_image.numpy(), axis=0), keypoints)\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(output_overlay)\n",
    "        _ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:120: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:120: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detector = PersonDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageProcessor = ImageProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(detector:PersonDetector,imageProcessor:ImageProcessor,path:str):\n",
    "    keypoint_data = []\n",
    "    image_label = []\n",
    "    angle_data = []\n",
    "    for images in os.listdir(path):\n",
    "        image = imageProcessor.process_image(os.path.join(path,images))\n",
    "        keypoints = detector.detect(image)\n",
    "        \n",
    "        key = keypoints[0][0]\n",
    "        \n",
    "        keypoint_data.append(np.ndarray.flatten(keypoints[0][0]))\n",
    "        image_label.append(path.split('/')[-2])\n",
    "        \n",
    "        angle = calculate_angle(key)\n",
    "        angle_data.append(angle)\n",
    "\n",
    "    angle_data = np.array(angle_data)\n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    result[\"data\"] = keypoint_data\n",
    "    result[\"label\"] = image_label\n",
    "    result[\"right knee\"] = angle_data.T[0]\n",
    "    result[\"left knee\"] = angle_data.T[1]\n",
    "    result[\"right hip\"] = angle_data.T[2]\n",
    "    result[\"left hip\"] = angle_data.T[3]\n",
    "    result[\"right elbow\"] = angle_data.T[4]\n",
    "    result[\"left elbow\"] = angle_data.T[5]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def calculate_angle(key):\n",
    "    \n",
    "    right_wrist = key[10]\n",
    "    left_wrist = key[9]\n",
    "    \n",
    "    right_elbow = key[8]\n",
    "    left_elbow = key[7]\n",
    "    \n",
    "    right_shoulder = key[6]\n",
    "    left_shoulder = key[5]\n",
    "    \n",
    "    right_hip = key[12]\n",
    "    left_hip = key[11]\n",
    "    \n",
    "    right_knee = key[14]\n",
    "    left_knee = key[13]\n",
    "    \n",
    "    right_ankle = key[16]\n",
    "    left_ankle = key[15]\n",
    "    \n",
    "    angle_right_we = np.degrees(np.arctan2(right_wrist[1] - right_elbow[1], right_wrist[0] - right_elbow[0]))\n",
    "    angle_right_es = np.degrees(np.arctan2(right_elbow[1] - right_shoulder[1], right_elbow[0] - right_shoulder[0]))\n",
    "    angle_right_sh = np.degrees(np.arctan2(right_shoulder[1] - right_hip[1], right_shoulder[0] - right_hip[0]))\n",
    "    angle_right_hk = np.degrees(np.arctan2(right_hip[1] - right_knee[1], right_hip[0] - right_knee[0]))\n",
    "    angle_right_ka = np.degrees(np.arctan2(right_knee[1] - right_ankle[1], right_knee[0] - right_ankle[0]))\n",
    "    \n",
    "    angle_left_we = np.degrees(np.arctan2(left_wrist[1] - left_elbow[1], left_wrist[0] - left_elbow[0]))\n",
    "    angle_left_es = np.degrees(np.arctan2(left_elbow[1] - left_shoulder[1], left_elbow[0] - left_shoulder[0]))\n",
    "    angle_left_sh = np.degrees(np.arctan2(left_shoulder[1] - left_hip[1], left_shoulder[0] - left_hip[0]))\n",
    "    angle_left_hk = np.degrees(np.arctan2(left_hip[1] - left_knee[1], left_hip[0] - left_knee[0]))\n",
    "    angle_left_ka = np.degrees(np.arctan2(left_knee[1] - left_ankle[1], left_knee[0] - left_ankle[0]))\n",
    "    \n",
    "    angles = [angle_right_we,angle_right_es,angle_right_sh,angle_right_hk,angle_right_ka,angle_left_we,angle_left_es,angle_left_sh,angle_left_hk,angle_left_ka]\n",
    "    angles = [angle if angle > 0 else angle + 180 for angle in angles]\n",
    "    \n",
    "    right_knee_angle = abs(angles[3] - angles[4])\n",
    "    left_knee_angle = abs(angles[8] - angles[9])\n",
    "    \n",
    "    right_hip_angle = abs(angles[2] - angles[3])\n",
    "    left_hip_angle = abs(angles[7] - angles[8])\n",
    "    \n",
    "    right_elbow_angle = abs(angles[0] - angles[1])\n",
    "    left_elbow_angle = abs(angles[5] - angles[6])\n",
    "    \n",
    "    angles = [right_knee_angle,left_knee_angle,right_hip_angle,left_hip_angle,right_elbow_angle,left_elbow_angle]\n",
    "    \n",
    "    return angles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pushup\n",
      "Saving pushup\n",
      "Processing situp\n",
      "Saving situp\n",
      "Processing squat\n",
      "Saving squat\n",
      "Processing standingFront\n",
      "Saving standingFront\n",
      "Processing standingSide\n",
      "Saving standingSide\n"
     ]
    }
   ],
   "source": [
    "path = \"../../Datasets/Trainer/processed/\"\n",
    "for folders in os.listdir(path):\n",
    "    print(f'Processing {folders}')\n",
    "    result = load_image(detector,imageProcessor,os.path.join(path,folders))\n",
    "    print(f'Saving {folders}')\n",
    "    result.to_csv(f\"../../Datasets/Trainer/csv/{folders}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../Datasets/Trainer/csv_raw/\"\n",
    "\n",
    "df1 = pd.read_csv(os.path.join(path,\"pushup.csv\"))\n",
    "df2 = pd.read_csv(os.path.join(path,\"situp.csv\"))\n",
    "df3 = pd.read_csv(os.path.join(path,\"squat.csv\"))\n",
    "df4 = pd.read_csv(os.path.join(path,\"standingFront.csv\"))\n",
    "df5 = pd.read_csv(os.path.join(path,\"standingSide.csv\"))\n",
    "\n",
    "dfs = [df1,df2,df3,df4,df5]\n",
    "for df in dfs:\n",
    "    df[\"angle\"] = df[df.columns[2:]].apply(lambda x: ','.join(x.dropna().astype(str)),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.concat([df1,df2,df3,df4,df5],ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "x_train,x_test,y_train,y_test,yAngle_train,yAngle_test = train_test_split(df[\"data\"],df[\"label\"],df[\"angle\"],test_size=0.2,random_state=42)\n",
    "x_train,x_val,y_train,y_val,yAngle_train,yAngle_val = train_test_split(x_train,y_train,yAngle_train,test_size=0.1,random_state=42)\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_df[\"data\"] = x_train\n",
    "train_df[\"label\"] = y_train\n",
    "train_df[\"angle\"] = yAngle_train\n",
    "\n",
    "val_df = pd.DataFrame()\n",
    "val_df[\"data\"] = x_val\n",
    "val_df[\"label\"] = y_val\n",
    "val_df[\"angle\"] = yAngle_val\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "test_df[\"data\"] = x_test\n",
    "test_df[\"label\"] = y_test\n",
    "test_df[\"angle\"] = yAngle_test\n",
    "\n",
    "train_df.to_csv(\"../../Datasets/Trainer/csv_processed/train.csv\",index=False)\n",
    "val_df.to_csv(\"../../Datasets/Trainer/csv_processed/val.csv\",index=False)\n",
    "test_df.to_csv(\"../../Datasets/Trainer/csv_processed/test.csv\",index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "      <th>angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5253</th>\n",
       "      <td>[0.28061217 0.53466886 0.5707864  0.26465243 0...</td>\n",
       "      <td>situp</td>\n",
       "      <td>94.94265365600586,94.76234817504884,114.650081...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>[0.3657635  0.24923955 0.54567796 0.3520516  0...</td>\n",
       "      <td>pushup</td>\n",
       "      <td>12.392730712890623,10.10626220703125,4.4919967...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>[0.50792116 0.7084065  0.62425554 0.5026107  0...</td>\n",
       "      <td>situp</td>\n",
       "      <td>111.23760223388672,108.94094848632812,76.97820...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>[0.2906944  0.1830423  0.8032792  0.27036238 0...</td>\n",
       "      <td>pushup</td>\n",
       "      <td>7.973281860351562,8.118682861328125,17.1819763...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5601</th>\n",
       "      <td>[0.3099822  0.41131708 0.5496848  0.29331765 0...</td>\n",
       "      <td>situp</td>\n",
       "      <td>96.87570190429688,96.88082122802734,33.2161979...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4044</th>\n",
       "      <td>[0.39310536 0.39399818 0.4987116  0.37454075 0...</td>\n",
       "      <td>situp</td>\n",
       "      <td>109.66102600097656,113.5357666015625,33.529468...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>[0.55826414 0.75834334 0.30647957 0.5518789  0...</td>\n",
       "      <td>situp</td>\n",
       "      <td>60.49694061279297,71.74858093261719,49.5243225...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>[0.5672023  0.34723738 0.42272753 0.5672327  0...</td>\n",
       "      <td>situp</td>\n",
       "      <td>115.13079452514648,110.42207717895508,49.65220...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5423</th>\n",
       "      <td>[0.3761958  0.45785332 0.54050326 0.3616412  0...</td>\n",
       "      <td>situp</td>\n",
       "      <td>66.36786651611328,71.7598648071289,111.4508895...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>[0.03557435 0.7497714  0.6662984  0.00450381 0...</td>\n",
       "      <td>pushup</td>\n",
       "      <td>0.686676025390625,1.467864990234375,4.44563293...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5139 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   data   label   \n",
       "5253  [0.28061217 0.53466886 0.5707864  0.26465243 0...   situp  \\\n",
       "717   [0.3657635  0.24923955 0.54567796 0.3520516  0...  pushup   \n",
       "4008  [0.50792116 0.7084065  0.62425554 0.5026107  0...   situp   \n",
       "2479  [0.2906944  0.1830423  0.8032792  0.27036238 0...  pushup   \n",
       "5601  [0.3099822  0.41131708 0.5496848  0.29331765 0...   situp   \n",
       "...                                                 ...     ...   \n",
       "4044  [0.39310536 0.39399818 0.4987116  0.37454075 0...   situp   \n",
       "3675  [0.55826414 0.75834334 0.30647957 0.5518789  0...   situp   \n",
       "3769  [0.5672023  0.34723738 0.42272753 0.5672327  0...   situp   \n",
       "5423  [0.3761958  0.45785332 0.54050326 0.3616412  0...   situp   \n",
       "1204  [0.03557435 0.7497714  0.6662984  0.00450381 0...  pushup   \n",
       "\n",
       "                                                  angle  \n",
       "5253  94.94265365600586,94.76234817504884,114.650081...  \n",
       "717   12.392730712890623,10.10626220703125,4.4919967...  \n",
       "4008  111.23760223388672,108.94094848632812,76.97820...  \n",
       "2479  7.973281860351562,8.118682861328125,17.1819763...  \n",
       "5601  96.87570190429688,96.88082122802734,33.2161979...  \n",
       "...                                                 ...  \n",
       "4044  109.66102600097656,113.5357666015625,33.529468...  \n",
       "3675  60.49694061279297,71.74858093261719,49.5243225...  \n",
       "3769  115.13079452514648,110.42207717895508,49.65220...  \n",
       "5423  66.36786651611328,71.7598648071289,111.4508895...  \n",
       "1204  0.686676025390625,1.467864990234375,4.44563293...  \n",
       "\n",
       "[5139 rows x 3 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pose_landmarks(train_path,val_path,test_path):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    val_df = pd.read_csv(val_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    X_train = []\n",
    "    X_val = []\n",
    "    X_test = []\n",
    "    temp_x_train = train_df.pop(\"data\")\n",
    "    temp_x_val = val_df.pop(\"data\")\n",
    "    temp_x_test = test_df.pop(\"data\")\n",
    "    \n",
    "    for data in temp_x_train:\n",
    "        spliited = data.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\n\",\"\").replace(\"  \",\" \").split(\" \")\n",
    "        data = np.array([float(i) for i in spliited if i != \"\"])\n",
    "        data = data.astype(np.float64)\n",
    "        X_train.append(data)\n",
    "        \n",
    "    for data in temp_x_val:\n",
    "        spliited = data.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\n\",\"\").replace(\"  \",\" \").split(\" \")\n",
    "        data = np.array([float(i) for i in spliited if i != \"\"])\n",
    "        data = data.astype(np.float64)\n",
    "        X_val.append(data)\n",
    "    \n",
    "    for data in temp_x_test:\n",
    "        spliited = data.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\n\",\"\").replace(\"  \",\" \").split(\" \")\n",
    "        data = np.array([float(i) for i in spliited if i != \"\"])\n",
    "        data = data.astype(np.float64)\n",
    "        X_test.append(data)\n",
    "        \n",
    "    classes = train_df[\"label\"].unique()\n",
    "    train_df[\"label_no\"] = train_df[\"label\"].map({classes[0]:0,classes[1]:1,classes[2]:2,classes[3]:3,classes[4]:4})\n",
    "    val_df[\"label_no\"] = val_df[\"label\"].map({classes[0]:0,classes[1]:1,classes[2]:2,classes[3]:3,classes[4]:4})\n",
    "    test_df[\"label_no\"] = test_df[\"label\"].map({classes[0]:0,classes[1]:1,classes[2]:2,classes[3]:3,classes[4]:4})\n",
    "    \n",
    "    y_train = tf.keras.utils.to_categorical(train_df.pop(\"label_no\"))\n",
    "    y_val = tf.keras.utils.to_categorical(val_df.pop(\"label_no\"))\n",
    "    y_test = tf.keras.utils.to_categorical(test_df.pop(\"label_no\"))\n",
    "    \n",
    "    temp_yAngle_train = train_df.pop(\"angle\")\n",
    "    temp_yAngle_val = val_df.pop(\"angle\")\n",
    "    temp_yAngle_test = test_df.pop(\"angle\")\n",
    "    yAngle_train = []\n",
    "    yAngle_val = []\n",
    "    yAngle_test = []\n",
    "    \n",
    "    for data in temp_yAngle_train:\n",
    "        spliited = data.split(\",\")\n",
    "        data = np.array([float(i) for i in spliited if i != \"\"])\n",
    "        data = data.astype(np.float64)\n",
    "        yAngle_train.append(data)\n",
    "    \n",
    "    for data in temp_yAngle_val:\n",
    "        spliited = data.split(\",\")\n",
    "        data = np.array([float(i) for i in spliited if i != \"\"])\n",
    "        data = data.astype(np.float64)\n",
    "        yAngle_val.append(data)\n",
    "        \n",
    "    for data in temp_yAngle_test:\n",
    "        spliited = data.split(\",\")\n",
    "        data = np.array([float(i) for i in spliited if i != \"\"])\n",
    "        data = data.astype(np.float64)\n",
    "        yAngle_test.append(data)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    X_train = tf.stack(X_train)\n",
    "    X_val = tf.stack(X_val)\n",
    "    X_test = tf.stack(X_test)\n",
    "    y_train = tf.stack(y_train)\n",
    "    y_val = tf.stack(y_val)\n",
    "    y_test = tf.stack(y_test)\n",
    "    yAngle_train = tf.stack(yAngle_train)\n",
    "    yAngle_val = tf.stack(yAngle_val)\n",
    "    yAngle_test = tf.stack(yAngle_test)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    return X_train,X_val,X_test,y_train,y_val,y_test,classes,yAngle_train,yAngle_val,yAngle_test\n",
    "\n",
    "X_train,X_val,X_test,y_train,y_val,y_test,classes,yAngle_train,yAngle_val,yAngle_test = pose_landmarks(\"../../Datasets/Trainer/csv_processed/train.csv\",\"../../Datasets/Trainer/csv_processed/val.csv\",\"../../Datasets/Trainer/csv_processed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " data_input (InputLayer)     [(None, 51)]              0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 128)               6656      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 64)                16448     \n",
      "                                                                 \n",
      " pose (Dense)                (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72965 (285.02 KB)\n",
      "Trainable params: 72965 (285.02 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def landmark_to_embedding(landmark):\n",
    "    \n",
    "    embedding = tf.keras.layers.Reshape((17, 3))(landmark)\n",
    "    return embedding\n",
    "\n",
    "inputs = tf.keras.Input(shape=(51),name=\"data_input\")\n",
    "layer = tf.keras.layers.Dense(128, activation=tf.nn.relu6)(inputs)\n",
    "layer = tf.keras.layers.Dropout(0.5)(layer)\n",
    "layer = tf.keras.layers.Dense(128, activation=tf.nn.relu6)(layer)\n",
    "layer = tf.keras.layers.Dense(256, activation=tf.nn.relu6)(layer)\n",
    "layer = tf.keras.layers.Dropout(0.5)(layer)\n",
    "layer = tf.keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\n",
    "# layer2 = tf.keras.layers.Dense(256, activation=tf.nn.relu6)(layer)\n",
    "outputs_1 = tf.keras.layers.Dense(5, activation=\"softmax\",name=\"pose\")(layer)\n",
    "#\n",
    "# outputs_3 = tf.keras.layers.Dense(6,name=\"angle\")(layer2)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs_1)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:5575: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 3s 6ms/step - loss: 1.2236 - accuracy: 0.4363 - val_loss: 1.1332 - val_accuracy: 0.6200\n",
      "Epoch 2/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 1.0617 - accuracy: 0.5380 - val_loss: 0.9873 - val_accuracy: 0.7215\n",
      "Epoch 3/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.9041 - accuracy: 0.6673 - val_loss: 0.7395 - val_accuracy: 0.7636\n",
      "Epoch 4/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.7212 - accuracy: 0.7533 - val_loss: 0.5492 - val_accuracy: 0.7881\n",
      "Epoch 5/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.5542 - accuracy: 0.8085 - val_loss: 0.4279 - val_accuracy: 0.8406\n",
      "Epoch 6/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.4584 - accuracy: 0.8404 - val_loss: 0.3436 - val_accuracy: 0.8862\n",
      "Epoch 7/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3855 - accuracy: 0.8716 - val_loss: 0.3018 - val_accuracy: 0.9002\n",
      "Epoch 8/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.3425 - accuracy: 0.8939 - val_loss: 0.2736 - val_accuracy: 0.9072\n",
      "Epoch 9/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.3014 - accuracy: 0.9033 - val_loss: 0.2472 - val_accuracy: 0.9194\n",
      "Epoch 10/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.2843 - accuracy: 0.9113 - val_loss: 0.2394 - val_accuracy: 0.9194\n",
      "Epoch 11/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.2664 - accuracy: 0.9155 - val_loss: 0.2170 - val_accuracy: 0.9299\n",
      "Epoch 12/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.2469 - accuracy: 0.9198 - val_loss: 0.2061 - val_accuracy: 0.9370\n",
      "Epoch 13/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.2262 - accuracy: 0.9276 - val_loss: 0.2010 - val_accuracy: 0.9317\n",
      "Epoch 14/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.2117 - accuracy: 0.9296 - val_loss: 0.1911 - val_accuracy: 0.9352\n",
      "Epoch 15/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.2126 - accuracy: 0.9303 - val_loss: 0.1919 - val_accuracy: 0.9335\n",
      "Epoch 16/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.2071 - accuracy: 0.9325 - val_loss: 0.1798 - val_accuracy: 0.9370\n",
      "Epoch 17/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.1909 - accuracy: 0.9352 - val_loss: 0.1839 - val_accuracy: 0.9352\n",
      "Epoch 18/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.1896 - accuracy: 0.9358 - val_loss: 0.1717 - val_accuracy: 0.9422\n",
      "Epoch 19/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.1880 - accuracy: 0.9362 - val_loss: 0.1654 - val_accuracy: 0.9475\n",
      "Epoch 20/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.1784 - accuracy: 0.9377 - val_loss: 0.1680 - val_accuracy: 0.9387\n",
      "Epoch 21/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.1693 - accuracy: 0.9424 - val_loss: 0.1608 - val_accuracy: 0.9422\n",
      "Epoch 22/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.1745 - accuracy: 0.9424 - val_loss: 0.1600 - val_accuracy: 0.9457\n",
      "Epoch 23/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.1716 - accuracy: 0.9412 - val_loss: 0.1537 - val_accuracy: 0.9457\n",
      "Epoch 24/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.1613 - accuracy: 0.9440 - val_loss: 0.1454 - val_accuracy: 0.9475\n",
      "Epoch 25/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.1558 - accuracy: 0.9478 - val_loss: 0.1511 - val_accuracy: 0.9457\n",
      "Epoch 26/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.1513 - accuracy: 0.9453 - val_loss: 0.1602 - val_accuracy: 0.9422\n",
      "Epoch 27/1000\n",
      "161/161 [==============================] - 2s 11ms/step - loss: 0.1537 - accuracy: 0.9453 - val_loss: 0.1356 - val_accuracy: 0.9475\n",
      "Epoch 28/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.1565 - accuracy: 0.9453 - val_loss: 0.1376 - val_accuracy: 0.9475\n",
      "Epoch 29/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.1458 - accuracy: 0.9486 - val_loss: 0.1322 - val_accuracy: 0.9510\n",
      "Epoch 30/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1449 - accuracy: 0.9504 - val_loss: 0.1401 - val_accuracy: 0.9475\n",
      "Epoch 31/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1375 - accuracy: 0.9508 - val_loss: 0.1245 - val_accuracy: 0.9527\n",
      "Epoch 32/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.1307 - accuracy: 0.9547 - val_loss: 0.1320 - val_accuracy: 0.9545\n",
      "Epoch 33/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1394 - accuracy: 0.9506 - val_loss: 0.1293 - val_accuracy: 0.9475\n",
      "Epoch 34/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1362 - accuracy: 0.9498 - val_loss: 0.1320 - val_accuracy: 0.9492\n",
      "Epoch 35/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1318 - accuracy: 0.9537 - val_loss: 0.1224 - val_accuracy: 0.9527\n",
      "Epoch 36/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.1292 - accuracy: 0.9543 - val_loss: 0.1218 - val_accuracy: 0.9527\n",
      "Epoch 37/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1240 - accuracy: 0.9556 - val_loss: 0.1168 - val_accuracy: 0.9545\n",
      "Epoch 38/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.1301 - accuracy: 0.9531 - val_loss: 0.1135 - val_accuracy: 0.9545\n",
      "Epoch 39/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1294 - accuracy: 0.9556 - val_loss: 0.1177 - val_accuracy: 0.9545\n",
      "Epoch 40/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.1159 - accuracy: 0.9605 - val_loss: 0.1267 - val_accuracy: 0.9510\n",
      "Epoch 41/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1086 - accuracy: 0.9601 - val_loss: 0.1143 - val_accuracy: 0.9545\n",
      "Epoch 42/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1171 - accuracy: 0.9560 - val_loss: 0.1133 - val_accuracy: 0.9527\n",
      "Epoch 43/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1213 - accuracy: 0.9568 - val_loss: 0.1161 - val_accuracy: 0.9527\n",
      "Epoch 44/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.1233 - accuracy: 0.9574 - val_loss: 0.1173 - val_accuracy: 0.9545\n",
      "Epoch 45/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.1118 - accuracy: 0.9607 - val_loss: 0.1170 - val_accuracy: 0.9492\n",
      "Epoch 46/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.1198 - accuracy: 0.9562 - val_loss: 0.1045 - val_accuracy: 0.9562\n",
      "Epoch 47/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.1122 - accuracy: 0.9591 - val_loss: 0.1050 - val_accuracy: 0.9597\n",
      "Epoch 48/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1063 - accuracy: 0.9628 - val_loss: 0.1095 - val_accuracy: 0.9562\n",
      "Epoch 49/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1080 - accuracy: 0.9601 - val_loss: 0.0988 - val_accuracy: 0.9597\n",
      "Epoch 50/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.1081 - accuracy: 0.9632 - val_loss: 0.1018 - val_accuracy: 0.9597\n",
      "Epoch 51/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1104 - accuracy: 0.9605 - val_loss: 0.1029 - val_accuracy: 0.9562\n",
      "Epoch 52/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.1072 - accuracy: 0.9617 - val_loss: 0.1048 - val_accuracy: 0.9510\n",
      "Epoch 53/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1077 - accuracy: 0.9621 - val_loss: 0.0967 - val_accuracy: 0.9597\n",
      "Epoch 54/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1015 - accuracy: 0.9648 - val_loss: 0.1064 - val_accuracy: 0.9580\n",
      "Epoch 55/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1059 - accuracy: 0.9621 - val_loss: 0.0991 - val_accuracy: 0.9597\n",
      "Epoch 56/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0979 - accuracy: 0.9648 - val_loss: 0.1051 - val_accuracy: 0.9562\n",
      "Epoch 57/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0986 - accuracy: 0.9640 - val_loss: 0.0993 - val_accuracy: 0.9580\n",
      "Epoch 58/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.1010 - accuracy: 0.9656 - val_loss: 0.0959 - val_accuracy: 0.9615\n",
      "Epoch 59/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0977 - accuracy: 0.9630 - val_loss: 0.1070 - val_accuracy: 0.9580\n",
      "Epoch 60/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0951 - accuracy: 0.9654 - val_loss: 0.1014 - val_accuracy: 0.9615\n",
      "Epoch 61/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.1012 - accuracy: 0.9626 - val_loss: 0.0949 - val_accuracy: 0.9597\n",
      "Epoch 62/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0921 - accuracy: 0.9694 - val_loss: 0.0926 - val_accuracy: 0.9615\n",
      "Epoch 63/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0930 - accuracy: 0.9658 - val_loss: 0.0925 - val_accuracy: 0.9615\n",
      "Epoch 64/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0883 - accuracy: 0.9658 - val_loss: 0.0992 - val_accuracy: 0.9597\n",
      "Epoch 65/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0911 - accuracy: 0.9696 - val_loss: 0.0955 - val_accuracy: 0.9632\n",
      "Epoch 66/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0917 - accuracy: 0.9654 - val_loss: 0.0955 - val_accuracy: 0.9632\n",
      "Epoch 67/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0810 - accuracy: 0.9698 - val_loss: 0.0934 - val_accuracy: 0.9632\n",
      "Epoch 68/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0922 - accuracy: 0.9671 - val_loss: 0.1002 - val_accuracy: 0.9632\n",
      "Epoch 69/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0835 - accuracy: 0.9720 - val_loss: 0.0930 - val_accuracy: 0.9615\n",
      "Epoch 70/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0865 - accuracy: 0.9706 - val_loss: 0.0889 - val_accuracy: 0.9650\n",
      "Epoch 71/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0862 - accuracy: 0.9689 - val_loss: 0.0882 - val_accuracy: 0.9650\n",
      "Epoch 72/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0907 - accuracy: 0.9694 - val_loss: 0.0870 - val_accuracy: 0.9667\n",
      "Epoch 73/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0840 - accuracy: 0.9700 - val_loss: 0.0903 - val_accuracy: 0.9632\n",
      "Epoch 74/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0776 - accuracy: 0.9733 - val_loss: 0.0900 - val_accuracy: 0.9650\n",
      "Epoch 75/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0864 - accuracy: 0.9673 - val_loss: 0.0899 - val_accuracy: 0.9650\n",
      "Epoch 76/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0831 - accuracy: 0.9698 - val_loss: 0.0885 - val_accuracy: 0.9632\n",
      "Epoch 77/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0835 - accuracy: 0.9704 - val_loss: 0.0923 - val_accuracy: 0.9615\n",
      "Epoch 78/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0803 - accuracy: 0.9698 - val_loss: 0.0924 - val_accuracy: 0.9615\n",
      "Epoch 79/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0805 - accuracy: 0.9714 - val_loss: 0.0889 - val_accuracy: 0.9650\n",
      "Epoch 80/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0854 - accuracy: 0.9702 - val_loss: 0.0937 - val_accuracy: 0.9632\n",
      "Epoch 81/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0825 - accuracy: 0.9694 - val_loss: 0.0852 - val_accuracy: 0.9650\n",
      "Epoch 82/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0832 - accuracy: 0.9687 - val_loss: 0.0858 - val_accuracy: 0.9685\n",
      "Epoch 83/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0818 - accuracy: 0.9700 - val_loss: 0.0834 - val_accuracy: 0.9667\n",
      "Epoch 84/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0792 - accuracy: 0.9710 - val_loss: 0.0871 - val_accuracy: 0.9685\n",
      "Epoch 85/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0759 - accuracy: 0.9737 - val_loss: 0.0825 - val_accuracy: 0.9667\n",
      "Epoch 86/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0826 - accuracy: 0.9677 - val_loss: 0.0853 - val_accuracy: 0.9650\n",
      "Epoch 87/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0771 - accuracy: 0.9702 - val_loss: 0.0816 - val_accuracy: 0.9667\n",
      "Epoch 88/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0719 - accuracy: 0.9735 - val_loss: 0.0913 - val_accuracy: 0.9650\n",
      "Epoch 89/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0716 - accuracy: 0.9749 - val_loss: 0.0796 - val_accuracy: 0.9685\n",
      "Epoch 90/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0730 - accuracy: 0.9741 - val_loss: 0.0826 - val_accuracy: 0.9667\n",
      "Epoch 91/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0728 - accuracy: 0.9722 - val_loss: 0.0774 - val_accuracy: 0.9702\n",
      "Epoch 92/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0697 - accuracy: 0.9728 - val_loss: 0.0809 - val_accuracy: 0.9685\n",
      "Epoch 93/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0711 - accuracy: 0.9726 - val_loss: 0.0762 - val_accuracy: 0.9702\n",
      "Epoch 94/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0679 - accuracy: 0.9753 - val_loss: 0.0771 - val_accuracy: 0.9702\n",
      "Epoch 95/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0724 - accuracy: 0.9724 - val_loss: 0.0784 - val_accuracy: 0.9702\n",
      "Epoch 96/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0696 - accuracy: 0.9728 - val_loss: 0.0808 - val_accuracy: 0.9702\n",
      "Epoch 97/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0679 - accuracy: 0.9749 - val_loss: 0.0722 - val_accuracy: 0.9720\n",
      "Epoch 98/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0678 - accuracy: 0.9757 - val_loss: 0.0857 - val_accuracy: 0.9702\n",
      "Epoch 99/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0734 - accuracy: 0.9733 - val_loss: 0.0745 - val_accuracy: 0.9720\n",
      "Epoch 100/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0672 - accuracy: 0.9737 - val_loss: 0.0868 - val_accuracy: 0.9685\n",
      "Epoch 101/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0608 - accuracy: 0.9759 - val_loss: 0.0799 - val_accuracy: 0.9755\n",
      "Epoch 102/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0734 - accuracy: 0.9712 - val_loss: 0.0755 - val_accuracy: 0.9720\n",
      "Epoch 103/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0663 - accuracy: 0.9761 - val_loss: 0.0763 - val_accuracy: 0.9702\n",
      "Epoch 104/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0616 - accuracy: 0.9763 - val_loss: 0.0811 - val_accuracy: 0.9755\n",
      "Epoch 105/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0663 - accuracy: 0.9778 - val_loss: 0.0714 - val_accuracy: 0.9737\n",
      "Epoch 106/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0632 - accuracy: 0.9772 - val_loss: 0.0745 - val_accuracy: 0.9702\n",
      "Epoch 107/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0633 - accuracy: 0.9772 - val_loss: 0.0840 - val_accuracy: 0.9685\n",
      "Epoch 108/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0642 - accuracy: 0.9761 - val_loss: 0.0788 - val_accuracy: 0.9702\n",
      "Epoch 109/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0586 - accuracy: 0.9778 - val_loss: 0.0796 - val_accuracy: 0.9702\n",
      "Epoch 110/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0668 - accuracy: 0.9747 - val_loss: 0.0753 - val_accuracy: 0.9685\n",
      "Epoch 111/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0615 - accuracy: 0.9768 - val_loss: 0.0701 - val_accuracy: 0.9737\n",
      "Epoch 112/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0637 - accuracy: 0.9774 - val_loss: 0.0714 - val_accuracy: 0.9720\n",
      "Epoch 113/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0662 - accuracy: 0.9759 - val_loss: 0.0627 - val_accuracy: 0.9755\n",
      "Epoch 114/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0671 - accuracy: 0.9735 - val_loss: 0.0776 - val_accuracy: 0.9685\n",
      "Epoch 115/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0617 - accuracy: 0.9772 - val_loss: 0.0657 - val_accuracy: 0.9755\n",
      "Epoch 116/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0601 - accuracy: 0.9778 - val_loss: 0.0704 - val_accuracy: 0.9702\n",
      "Epoch 117/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0607 - accuracy: 0.9772 - val_loss: 0.0808 - val_accuracy: 0.9685\n",
      "Epoch 118/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0603 - accuracy: 0.9770 - val_loss: 0.0825 - val_accuracy: 0.9702\n",
      "Epoch 119/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0595 - accuracy: 0.9800 - val_loss: 0.0697 - val_accuracy: 0.9702\n",
      "Epoch 120/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0586 - accuracy: 0.9798 - val_loss: 0.0697 - val_accuracy: 0.9737\n",
      "Epoch 121/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0597 - accuracy: 0.9798 - val_loss: 0.0730 - val_accuracy: 0.9755\n",
      "Epoch 122/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0602 - accuracy: 0.9774 - val_loss: 0.0686 - val_accuracy: 0.9755\n",
      "Epoch 123/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0603 - accuracy: 0.9766 - val_loss: 0.0679 - val_accuracy: 0.9790\n",
      "Epoch 124/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0563 - accuracy: 0.9788 - val_loss: 0.0635 - val_accuracy: 0.9755\n",
      "Epoch 125/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0589 - accuracy: 0.9770 - val_loss: 0.0758 - val_accuracy: 0.9772\n",
      "Epoch 126/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0576 - accuracy: 0.9798 - val_loss: 0.0617 - val_accuracy: 0.9790\n",
      "Epoch 127/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0550 - accuracy: 0.9788 - val_loss: 0.0604 - val_accuracy: 0.9755\n",
      "Epoch 128/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0600 - accuracy: 0.9780 - val_loss: 0.0733 - val_accuracy: 0.9755\n",
      "Epoch 129/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0558 - accuracy: 0.9811 - val_loss: 0.0693 - val_accuracy: 0.9755\n",
      "Epoch 130/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0561 - accuracy: 0.9784 - val_loss: 0.0733 - val_accuracy: 0.9737\n",
      "Epoch 131/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0488 - accuracy: 0.9803 - val_loss: 0.0729 - val_accuracy: 0.9720\n",
      "Epoch 132/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0581 - accuracy: 0.9800 - val_loss: 0.0671 - val_accuracy: 0.9755\n",
      "Epoch 133/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0599 - accuracy: 0.9765 - val_loss: 0.0639 - val_accuracy: 0.9755\n",
      "Epoch 134/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0529 - accuracy: 0.9802 - val_loss: 0.0647 - val_accuracy: 0.9702\n",
      "Epoch 135/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0568 - accuracy: 0.9790 - val_loss: 0.0605 - val_accuracy: 0.9772\n",
      "Epoch 136/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0562 - accuracy: 0.9788 - val_loss: 0.0564 - val_accuracy: 0.9772\n",
      "Epoch 137/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0522 - accuracy: 0.9805 - val_loss: 0.0735 - val_accuracy: 0.9737\n",
      "Epoch 138/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0536 - accuracy: 0.9792 - val_loss: 0.0586 - val_accuracy: 0.9807\n",
      "Epoch 139/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0577 - accuracy: 0.9790 - val_loss: 0.0583 - val_accuracy: 0.9772\n",
      "Epoch 140/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0503 - accuracy: 0.9829 - val_loss: 0.0661 - val_accuracy: 0.9755\n",
      "Epoch 141/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0518 - accuracy: 0.9796 - val_loss: 0.0602 - val_accuracy: 0.9790\n",
      "Epoch 142/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0533 - accuracy: 0.9794 - val_loss: 0.0602 - val_accuracy: 0.9737\n",
      "Epoch 143/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0542 - accuracy: 0.9796 - val_loss: 0.0598 - val_accuracy: 0.9772\n",
      "Epoch 144/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0489 - accuracy: 0.9809 - val_loss: 0.0543 - val_accuracy: 0.9807\n",
      "Epoch 145/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0483 - accuracy: 0.9815 - val_loss: 0.0639 - val_accuracy: 0.9772\n",
      "Epoch 146/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0487 - accuracy: 0.9802 - val_loss: 0.0677 - val_accuracy: 0.9720\n",
      "Epoch 147/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0492 - accuracy: 0.9809 - val_loss: 0.0615 - val_accuracy: 0.9772\n",
      "Epoch 148/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0489 - accuracy: 0.9815 - val_loss: 0.0659 - val_accuracy: 0.9772\n",
      "Epoch 149/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0512 - accuracy: 0.9802 - val_loss: 0.0540 - val_accuracy: 0.9790\n",
      "Epoch 150/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0482 - accuracy: 0.9809 - val_loss: 0.0694 - val_accuracy: 0.9755\n",
      "Epoch 151/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0533 - accuracy: 0.9807 - val_loss: 0.0670 - val_accuracy: 0.9772\n",
      "Epoch 152/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0482 - accuracy: 0.9827 - val_loss: 0.0532 - val_accuracy: 0.9807\n",
      "Epoch 153/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0514 - accuracy: 0.9815 - val_loss: 0.0545 - val_accuracy: 0.9790\n",
      "Epoch 154/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0554 - accuracy: 0.9794 - val_loss: 0.0584 - val_accuracy: 0.9737\n",
      "Epoch 155/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0508 - accuracy: 0.9807 - val_loss: 0.0575 - val_accuracy: 0.9790\n",
      "Epoch 156/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0484 - accuracy: 0.9817 - val_loss: 0.0587 - val_accuracy: 0.9737\n",
      "Epoch 157/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0467 - accuracy: 0.9823 - val_loss: 0.0490 - val_accuracy: 0.9825\n",
      "Epoch 158/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0495 - accuracy: 0.9815 - val_loss: 0.0583 - val_accuracy: 0.9772\n",
      "Epoch 159/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0468 - accuracy: 0.9805 - val_loss: 0.0534 - val_accuracy: 0.9790\n",
      "Epoch 160/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0432 - accuracy: 0.9844 - val_loss: 0.0572 - val_accuracy: 0.9790\n",
      "Epoch 161/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0433 - accuracy: 0.9831 - val_loss: 0.0580 - val_accuracy: 0.9772\n",
      "Epoch 162/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0430 - accuracy: 0.9846 - val_loss: 0.0660 - val_accuracy: 0.9737\n",
      "Epoch 163/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0477 - accuracy: 0.9805 - val_loss: 0.0651 - val_accuracy: 0.9755\n",
      "Epoch 164/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0546 - accuracy: 0.9792 - val_loss: 0.0559 - val_accuracy: 0.9825\n",
      "Epoch 165/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0419 - accuracy: 0.9854 - val_loss: 0.0548 - val_accuracy: 0.9807\n",
      "Epoch 166/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0442 - accuracy: 0.9827 - val_loss: 0.0576 - val_accuracy: 0.9807\n",
      "Epoch 167/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0462 - accuracy: 0.9833 - val_loss: 0.0552 - val_accuracy: 0.9772\n",
      "Epoch 168/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0498 - accuracy: 0.9815 - val_loss: 0.0598 - val_accuracy: 0.9825\n",
      "Epoch 169/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0494 - accuracy: 0.9823 - val_loss: 0.0503 - val_accuracy: 0.9807\n",
      "Epoch 170/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0449 - accuracy: 0.9838 - val_loss: 0.0590 - val_accuracy: 0.9807\n",
      "Epoch 171/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0424 - accuracy: 0.9821 - val_loss: 0.0584 - val_accuracy: 0.9772\n",
      "Epoch 172/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0452 - accuracy: 0.9835 - val_loss: 0.0550 - val_accuracy: 0.9807\n",
      "Epoch 173/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0468 - accuracy: 0.9825 - val_loss: 0.0536 - val_accuracy: 0.9825\n",
      "Epoch 174/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0468 - accuracy: 0.9813 - val_loss: 0.0491 - val_accuracy: 0.9807\n",
      "Epoch 175/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0395 - accuracy: 0.9858 - val_loss: 0.0518 - val_accuracy: 0.9825\n",
      "Epoch 176/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0444 - accuracy: 0.9838 - val_loss: 0.0576 - val_accuracy: 0.9772\n",
      "Epoch 177/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0465 - accuracy: 0.9831 - val_loss: 0.0553 - val_accuracy: 0.9755\n",
      "Epoch 178/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0446 - accuracy: 0.9840 - val_loss: 0.0552 - val_accuracy: 0.9790\n",
      "Epoch 179/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0424 - accuracy: 0.9862 - val_loss: 0.0514 - val_accuracy: 0.9807\n",
      "Epoch 180/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0450 - accuracy: 0.9846 - val_loss: 0.0535 - val_accuracy: 0.9790\n",
      "Epoch 181/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0365 - accuracy: 0.9856 - val_loss: 0.0504 - val_accuracy: 0.9790\n",
      "Epoch 182/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0457 - accuracy: 0.9827 - val_loss: 0.0553 - val_accuracy: 0.9772\n",
      "Epoch 183/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0427 - accuracy: 0.9833 - val_loss: 0.0555 - val_accuracy: 0.9790\n",
      "Epoch 184/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0409 - accuracy: 0.9858 - val_loss: 0.0419 - val_accuracy: 0.9860\n",
      "Epoch 185/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0438 - accuracy: 0.9844 - val_loss: 0.0503 - val_accuracy: 0.9807\n",
      "Epoch 186/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0450 - accuracy: 0.9833 - val_loss: 0.0509 - val_accuracy: 0.9825\n",
      "Epoch 187/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0386 - accuracy: 0.9852 - val_loss: 0.0533 - val_accuracy: 0.9807\n",
      "Epoch 188/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0404 - accuracy: 0.9850 - val_loss: 0.0424 - val_accuracy: 0.9807\n",
      "Epoch 189/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0460 - accuracy: 0.9813 - val_loss: 0.0529 - val_accuracy: 0.9807\n",
      "Epoch 190/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0471 - accuracy: 0.9829 - val_loss: 0.0490 - val_accuracy: 0.9772\n",
      "Epoch 191/1000\n",
      "161/161 [==============================] - 1s 9ms/step - loss: 0.0411 - accuracy: 0.9844 - val_loss: 0.0525 - val_accuracy: 0.9772\n",
      "Epoch 192/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0390 - accuracy: 0.9868 - val_loss: 0.0562 - val_accuracy: 0.9825\n",
      "Epoch 193/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0416 - accuracy: 0.9852 - val_loss: 0.0627 - val_accuracy: 0.9807\n",
      "Epoch 194/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0432 - accuracy: 0.9838 - val_loss: 0.0497 - val_accuracy: 0.9842\n",
      "Epoch 195/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0420 - accuracy: 0.9872 - val_loss: 0.0481 - val_accuracy: 0.9860\n",
      "Epoch 196/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0390 - accuracy: 0.9870 - val_loss: 0.0489 - val_accuracy: 0.9825\n",
      "Epoch 197/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0384 - accuracy: 0.9856 - val_loss: 0.0467 - val_accuracy: 0.9842\n",
      "Epoch 198/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0349 - accuracy: 0.9872 - val_loss: 0.0558 - val_accuracy: 0.9825\n",
      "Epoch 199/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0383 - accuracy: 0.9856 - val_loss: 0.0595 - val_accuracy: 0.9807\n",
      "Epoch 200/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0337 - accuracy: 0.9866 - val_loss: 0.0551 - val_accuracy: 0.9825\n",
      "Epoch 201/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0370 - accuracy: 0.9850 - val_loss: 0.0652 - val_accuracy: 0.9720\n",
      "Epoch 202/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0388 - accuracy: 0.9870 - val_loss: 0.0579 - val_accuracy: 0.9790\n",
      "Epoch 203/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0403 - accuracy: 0.9862 - val_loss: 0.0461 - val_accuracy: 0.9842\n",
      "Epoch 204/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0411 - accuracy: 0.9848 - val_loss: 0.0539 - val_accuracy: 0.9772\n",
      "Epoch 205/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0379 - accuracy: 0.9848 - val_loss: 0.0441 - val_accuracy: 0.9825\n",
      "Epoch 206/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0426 - accuracy: 0.9844 - val_loss: 0.0440 - val_accuracy: 0.9790\n",
      "Epoch 207/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0376 - accuracy: 0.9868 - val_loss: 0.0494 - val_accuracy: 0.9842\n",
      "Epoch 208/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0364 - accuracy: 0.9860 - val_loss: 0.0451 - val_accuracy: 0.9825\n",
      "Epoch 209/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0436 - accuracy: 0.9848 - val_loss: 0.0503 - val_accuracy: 0.9807\n",
      "Epoch 210/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0344 - accuracy: 0.9872 - val_loss: 0.0530 - val_accuracy: 0.9860\n",
      "Epoch 211/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0365 - accuracy: 0.9864 - val_loss: 0.0529 - val_accuracy: 0.9825\n",
      "Epoch 212/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0386 - accuracy: 0.9852 - val_loss: 0.0489 - val_accuracy: 0.9860\n",
      "Epoch 213/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0377 - accuracy: 0.9858 - val_loss: 0.0420 - val_accuracy: 0.9842\n",
      "Epoch 214/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0358 - accuracy: 0.9866 - val_loss: 0.0440 - val_accuracy: 0.9825\n",
      "Epoch 215/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0355 - accuracy: 0.9883 - val_loss: 0.0446 - val_accuracy: 0.9772\n",
      "Epoch 216/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0306 - accuracy: 0.9881 - val_loss: 0.0511 - val_accuracy: 0.9790\n",
      "Epoch 217/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0336 - accuracy: 0.9852 - val_loss: 0.0555 - val_accuracy: 0.9825\n",
      "Epoch 218/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0335 - accuracy: 0.9868 - val_loss: 0.0460 - val_accuracy: 0.9842\n",
      "Epoch 219/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0387 - accuracy: 0.9842 - val_loss: 0.0414 - val_accuracy: 0.9877\n",
      "Epoch 220/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0331 - accuracy: 0.9887 - val_loss: 0.0485 - val_accuracy: 0.9807\n",
      "Epoch 221/1000\n",
      "161/161 [==============================] - 2s 10ms/step - loss: 0.0358 - accuracy: 0.9874 - val_loss: 0.0459 - val_accuracy: 0.9807\n",
      "Epoch 222/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0358 - accuracy: 0.9870 - val_loss: 0.0449 - val_accuracy: 0.9825\n",
      "Epoch 223/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0357 - accuracy: 0.9868 - val_loss: 0.0449 - val_accuracy: 0.9842\n",
      "Epoch 224/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0326 - accuracy: 0.9889 - val_loss: 0.0455 - val_accuracy: 0.9860\n",
      "Epoch 225/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0375 - accuracy: 0.9868 - val_loss: 0.0490 - val_accuracy: 0.9807\n",
      "Epoch 226/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0350 - accuracy: 0.9864 - val_loss: 0.0407 - val_accuracy: 0.9807\n",
      "Epoch 227/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0342 - accuracy: 0.9877 - val_loss: 0.0425 - val_accuracy: 0.9825\n",
      "Epoch 228/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0358 - accuracy: 0.9879 - val_loss: 0.0437 - val_accuracy: 0.9860\n",
      "Epoch 229/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0314 - accuracy: 0.9891 - val_loss: 0.0565 - val_accuracy: 0.9807\n",
      "Epoch 230/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0371 - accuracy: 0.9831 - val_loss: 0.0435 - val_accuracy: 0.9842\n",
      "Epoch 231/1000\n",
      "161/161 [==============================] - 2s 11ms/step - loss: 0.0311 - accuracy: 0.9889 - val_loss: 0.0461 - val_accuracy: 0.9860\n",
      "Epoch 232/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0373 - accuracy: 0.9874 - val_loss: 0.0416 - val_accuracy: 0.9860\n",
      "Epoch 233/1000\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.0369 - accuracy: 0.9860 - val_loss: 0.0365 - val_accuracy: 0.9842\n",
      "Epoch 234/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0354 - accuracy: 0.9866 - val_loss: 0.0466 - val_accuracy: 0.9860\n",
      "Epoch 235/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0296 - accuracy: 0.9893 - val_loss: 0.0396 - val_accuracy: 0.9860\n",
      "Epoch 236/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0335 - accuracy: 0.9891 - val_loss: 0.0502 - val_accuracy: 0.9825\n",
      "Epoch 237/1000\n",
      "161/161 [==============================] - 2s 10ms/step - loss: 0.0359 - accuracy: 0.9864 - val_loss: 0.0454 - val_accuracy: 0.9842\n",
      "Epoch 238/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0331 - accuracy: 0.9877 - val_loss: 0.0413 - val_accuracy: 0.9860\n",
      "Epoch 239/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0297 - accuracy: 0.9887 - val_loss: 0.0470 - val_accuracy: 0.9842\n",
      "Epoch 240/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0300 - accuracy: 0.9899 - val_loss: 0.0522 - val_accuracy: 0.9807\n",
      "Epoch 241/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0325 - accuracy: 0.9891 - val_loss: 0.0371 - val_accuracy: 0.9895\n",
      "Epoch 242/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0310 - accuracy: 0.9897 - val_loss: 0.0411 - val_accuracy: 0.9860\n",
      "Epoch 243/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0259 - accuracy: 0.9912 - val_loss: 0.0462 - val_accuracy: 0.9877\n",
      "Epoch 244/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0333 - accuracy: 0.9881 - val_loss: 0.0416 - val_accuracy: 0.9877\n",
      "Epoch 245/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0347 - accuracy: 0.9858 - val_loss: 0.0408 - val_accuracy: 0.9895\n",
      "Epoch 246/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0313 - accuracy: 0.9885 - val_loss: 0.0433 - val_accuracy: 0.9825\n",
      "Epoch 247/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0319 - accuracy: 0.9897 - val_loss: 0.0400 - val_accuracy: 0.9895\n",
      "Epoch 248/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0311 - accuracy: 0.9868 - val_loss: 0.0487 - val_accuracy: 0.9860\n",
      "Epoch 249/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0251 - accuracy: 0.9907 - val_loss: 0.0404 - val_accuracy: 0.9895\n",
      "Epoch 250/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0314 - accuracy: 0.9877 - val_loss: 0.0472 - val_accuracy: 0.9860\n",
      "Epoch 251/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0387 - accuracy: 0.9858 - val_loss: 0.0418 - val_accuracy: 0.9860\n",
      "Epoch 252/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0286 - accuracy: 0.9893 - val_loss: 0.0422 - val_accuracy: 0.9877\n",
      "Epoch 253/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0333 - accuracy: 0.9883 - val_loss: 0.0335 - val_accuracy: 0.9912\n",
      "Epoch 254/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0336 - accuracy: 0.9862 - val_loss: 0.0429 - val_accuracy: 0.9860\n",
      "Epoch 255/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0311 - accuracy: 0.9883 - val_loss: 0.0491 - val_accuracy: 0.9860\n",
      "Epoch 256/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0356 - accuracy: 0.9874 - val_loss: 0.0422 - val_accuracy: 0.9860\n",
      "Epoch 257/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0340 - accuracy: 0.9870 - val_loss: 0.0412 - val_accuracy: 0.9877\n",
      "Epoch 258/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0310 - accuracy: 0.9897 - val_loss: 0.0343 - val_accuracy: 0.9895\n",
      "Epoch 259/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0296 - accuracy: 0.9903 - val_loss: 0.0359 - val_accuracy: 0.9895\n",
      "Epoch 260/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0265 - accuracy: 0.9899 - val_loss: 0.0398 - val_accuracy: 0.9877\n",
      "Epoch 261/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0324 - accuracy: 0.9891 - val_loss: 0.0457 - val_accuracy: 0.9842\n",
      "Epoch 262/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0283 - accuracy: 0.9901 - val_loss: 0.0411 - val_accuracy: 0.9877\n",
      "Epoch 263/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0316 - accuracy: 0.9874 - val_loss: 0.0377 - val_accuracy: 0.9877\n",
      "Epoch 264/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0297 - accuracy: 0.9881 - val_loss: 0.0358 - val_accuracy: 0.9912\n",
      "Epoch 265/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0270 - accuracy: 0.9897 - val_loss: 0.0326 - val_accuracy: 0.9912\n",
      "Epoch 266/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0352 - accuracy: 0.9874 - val_loss: 0.0440 - val_accuracy: 0.9860\n",
      "Epoch 267/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0280 - accuracy: 0.9895 - val_loss: 0.0417 - val_accuracy: 0.9877\n",
      "Epoch 268/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0288 - accuracy: 0.9895 - val_loss: 0.0426 - val_accuracy: 0.9877\n",
      "Epoch 269/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0295 - accuracy: 0.9885 - val_loss: 0.0397 - val_accuracy: 0.9895\n",
      "Epoch 270/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0268 - accuracy: 0.9907 - val_loss: 0.0441 - val_accuracy: 0.9895\n",
      "Epoch 271/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0291 - accuracy: 0.9897 - val_loss: 0.0389 - val_accuracy: 0.9860\n",
      "Epoch 272/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0288 - accuracy: 0.9895 - val_loss: 0.0440 - val_accuracy: 0.9860\n",
      "Epoch 273/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0250 - accuracy: 0.9905 - val_loss: 0.0401 - val_accuracy: 0.9842\n",
      "Epoch 274/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0284 - accuracy: 0.9889 - val_loss: 0.0318 - val_accuracy: 0.9895\n",
      "Epoch 275/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0311 - accuracy: 0.9879 - val_loss: 0.0370 - val_accuracy: 0.9895\n",
      "Epoch 276/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0257 - accuracy: 0.9895 - val_loss: 0.0455 - val_accuracy: 0.9807\n",
      "Epoch 277/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0254 - accuracy: 0.9903 - val_loss: 0.0373 - val_accuracy: 0.9860\n",
      "Epoch 278/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0262 - accuracy: 0.9895 - val_loss: 0.0508 - val_accuracy: 0.9772\n",
      "Epoch 279/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0251 - accuracy: 0.9907 - val_loss: 0.0341 - val_accuracy: 0.9895\n",
      "Epoch 280/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0263 - accuracy: 0.9887 - val_loss: 0.0383 - val_accuracy: 0.9895\n",
      "Epoch 281/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0268 - accuracy: 0.9910 - val_loss: 0.0358 - val_accuracy: 0.9912\n",
      "Epoch 282/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0291 - accuracy: 0.9891 - val_loss: 0.0422 - val_accuracy: 0.9877\n",
      "Epoch 283/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0283 - accuracy: 0.9893 - val_loss: 0.0514 - val_accuracy: 0.9825\n",
      "Epoch 284/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0277 - accuracy: 0.9889 - val_loss: 0.0374 - val_accuracy: 0.9895\n",
      "Epoch 285/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0256 - accuracy: 0.9912 - val_loss: 0.0299 - val_accuracy: 0.9930\n",
      "Epoch 286/1000\n",
      "161/161 [==============================] - 1s 9ms/step - loss: 0.0278 - accuracy: 0.9895 - val_loss: 0.0403 - val_accuracy: 0.9877\n",
      "Epoch 287/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0261 - accuracy: 0.9897 - val_loss: 0.0410 - val_accuracy: 0.9895\n",
      "Epoch 288/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0278 - accuracy: 0.9909 - val_loss: 0.0362 - val_accuracy: 0.9895\n",
      "Epoch 289/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0241 - accuracy: 0.9916 - val_loss: 0.0344 - val_accuracy: 0.9912\n",
      "Epoch 290/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0304 - accuracy: 0.9897 - val_loss: 0.0434 - val_accuracy: 0.9877\n",
      "Epoch 291/1000\n",
      "161/161 [==============================] - 1s 9ms/step - loss: 0.0288 - accuracy: 0.9907 - val_loss: 0.0482 - val_accuracy: 0.9842\n",
      "Epoch 292/1000\n",
      "161/161 [==============================] - 2s 10ms/step - loss: 0.0241 - accuracy: 0.9903 - val_loss: 0.0369 - val_accuracy: 0.9877\n",
      "Epoch 293/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0258 - accuracy: 0.9916 - val_loss: 0.0389 - val_accuracy: 0.9877\n",
      "Epoch 294/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0276 - accuracy: 0.9895 - val_loss: 0.0356 - val_accuracy: 0.9912\n",
      "Epoch 295/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0226 - accuracy: 0.9928 - val_loss: 0.0443 - val_accuracy: 0.9860\n",
      "Epoch 296/1000\n",
      "161/161 [==============================] - 1s 9ms/step - loss: 0.0225 - accuracy: 0.9918 - val_loss: 0.0437 - val_accuracy: 0.9860\n",
      "Epoch 297/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0283 - accuracy: 0.9901 - val_loss: 0.0393 - val_accuracy: 0.9877\n",
      "Epoch 298/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0273 - accuracy: 0.9891 - val_loss: 0.0389 - val_accuracy: 0.9825\n",
      "Epoch 299/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0268 - accuracy: 0.9905 - val_loss: 0.0377 - val_accuracy: 0.9860\n",
      "Epoch 300/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0248 - accuracy: 0.9901 - val_loss: 0.0401 - val_accuracy: 0.9877\n",
      "Epoch 301/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0276 - accuracy: 0.9901 - val_loss: 0.0464 - val_accuracy: 0.9842\n",
      "Epoch 302/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0337 - accuracy: 0.9879 - val_loss: 0.0400 - val_accuracy: 0.9860\n",
      "Epoch 303/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0302 - accuracy: 0.9883 - val_loss: 0.0295 - val_accuracy: 0.9930\n",
      "Epoch 304/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0240 - accuracy: 0.9924 - val_loss: 0.0328 - val_accuracy: 0.9930\n",
      "Epoch 305/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0254 - accuracy: 0.9907 - val_loss: 0.0299 - val_accuracy: 0.9912\n",
      "Epoch 306/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0254 - accuracy: 0.9899 - val_loss: 0.0360 - val_accuracy: 0.9895\n",
      "Epoch 307/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0223 - accuracy: 0.9926 - val_loss: 0.0431 - val_accuracy: 0.9842\n",
      "Epoch 308/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0226 - accuracy: 0.9901 - val_loss: 0.0298 - val_accuracy: 0.9895\n",
      "Epoch 309/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0263 - accuracy: 0.9916 - val_loss: 0.0320 - val_accuracy: 0.9895\n",
      "Epoch 310/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0275 - accuracy: 0.9905 - val_loss: 0.0363 - val_accuracy: 0.9860\n",
      "Epoch 311/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0210 - accuracy: 0.9922 - val_loss: 0.0308 - val_accuracy: 0.9912\n",
      "Epoch 312/1000\n",
      "161/161 [==============================] - 2s 12ms/step - loss: 0.0235 - accuracy: 0.9914 - val_loss: 0.0333 - val_accuracy: 0.9877\n",
      "Epoch 313/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0261 - accuracy: 0.9905 - val_loss: 0.0420 - val_accuracy: 0.9860\n",
      "Epoch 314/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0220 - accuracy: 0.9924 - val_loss: 0.0401 - val_accuracy: 0.9860\n",
      "Epoch 315/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0243 - accuracy: 0.9914 - val_loss: 0.0398 - val_accuracy: 0.9912\n",
      "Epoch 316/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0275 - accuracy: 0.9901 - val_loss: 0.0355 - val_accuracy: 0.9895\n",
      "Epoch 317/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0219 - accuracy: 0.9916 - val_loss: 0.0387 - val_accuracy: 0.9877\n",
      "Epoch 318/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0191 - accuracy: 0.9914 - val_loss: 0.0393 - val_accuracy: 0.9877\n",
      "Epoch 319/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0239 - accuracy: 0.9918 - val_loss: 0.0394 - val_accuracy: 0.9877\n",
      "Epoch 320/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0251 - accuracy: 0.9916 - val_loss: 0.0415 - val_accuracy: 0.9895\n",
      "Epoch 321/1000\n",
      "161/161 [==============================] - 1s 9ms/step - loss: 0.0211 - accuracy: 0.9934 - val_loss: 0.0423 - val_accuracy: 0.9877\n",
      "Epoch 322/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0242 - accuracy: 0.9920 - val_loss: 0.0341 - val_accuracy: 0.9912\n",
      "Epoch 323/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0290 - accuracy: 0.9891 - val_loss: 0.0304 - val_accuracy: 0.9912\n",
      "Epoch 324/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0216 - accuracy: 0.9938 - val_loss: 0.0351 - val_accuracy: 0.9877\n",
      "Epoch 325/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0227 - accuracy: 0.9912 - val_loss: 0.0295 - val_accuracy: 0.9912\n",
      "Epoch 326/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0224 - accuracy: 0.9914 - val_loss: 0.0462 - val_accuracy: 0.9895\n",
      "Epoch 327/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0220 - accuracy: 0.9914 - val_loss: 0.0370 - val_accuracy: 0.9895\n",
      "Epoch 328/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0210 - accuracy: 0.9903 - val_loss: 0.0307 - val_accuracy: 0.9930\n",
      "Epoch 329/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0227 - accuracy: 0.9926 - val_loss: 0.0315 - val_accuracy: 0.9912\n",
      "Epoch 330/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0198 - accuracy: 0.9928 - val_loss: 0.0309 - val_accuracy: 0.9912\n",
      "Epoch 331/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0265 - accuracy: 0.9905 - val_loss: 0.0389 - val_accuracy: 0.9877\n",
      "Epoch 332/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0253 - accuracy: 0.9905 - val_loss: 0.0335 - val_accuracy: 0.9930\n",
      "Epoch 333/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0202 - accuracy: 0.9938 - val_loss: 0.0377 - val_accuracy: 0.9877\n",
      "Epoch 334/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0215 - accuracy: 0.9910 - val_loss: 0.0406 - val_accuracy: 0.9877\n",
      "Epoch 335/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0212 - accuracy: 0.9932 - val_loss: 0.0404 - val_accuracy: 0.9860\n",
      "Epoch 336/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0246 - accuracy: 0.9926 - val_loss: 0.0357 - val_accuracy: 0.9877\n",
      "Epoch 337/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0199 - accuracy: 0.9926 - val_loss: 0.0326 - val_accuracy: 0.9912\n",
      "Epoch 338/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0234 - accuracy: 0.9899 - val_loss: 0.0391 - val_accuracy: 0.9860\n",
      "Epoch 339/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0253 - accuracy: 0.9912 - val_loss: 0.0412 - val_accuracy: 0.9860\n",
      "Epoch 340/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0188 - accuracy: 0.9924 - val_loss: 0.0406 - val_accuracy: 0.9877\n",
      "Epoch 341/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0247 - accuracy: 0.9918 - val_loss: 0.0408 - val_accuracy: 0.9877\n",
      "Epoch 342/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0244 - accuracy: 0.9909 - val_loss: 0.0407 - val_accuracy: 0.9895\n",
      "Epoch 343/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0229 - accuracy: 0.9924 - val_loss: 0.0348 - val_accuracy: 0.9895\n",
      "Epoch 344/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0224 - accuracy: 0.9912 - val_loss: 0.0348 - val_accuracy: 0.9877\n",
      "Epoch 345/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0249 - accuracy: 0.9909 - val_loss: 0.0323 - val_accuracy: 0.9895\n",
      "Epoch 346/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0215 - accuracy: 0.9909 - val_loss: 0.0399 - val_accuracy: 0.9877\n",
      "Epoch 347/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0233 - accuracy: 0.9916 - val_loss: 0.0325 - val_accuracy: 0.9895\n",
      "Epoch 348/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0228 - accuracy: 0.9920 - val_loss: 0.0365 - val_accuracy: 0.9877\n",
      "Epoch 349/1000\n",
      "161/161 [==============================] - 1s 9ms/step - loss: 0.0220 - accuracy: 0.9926 - val_loss: 0.0351 - val_accuracy: 0.9895\n",
      "Epoch 350/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0220 - accuracy: 0.9916 - val_loss: 0.0331 - val_accuracy: 0.9877\n",
      "Epoch 351/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0222 - accuracy: 0.9922 - val_loss: 0.0380 - val_accuracy: 0.9912\n",
      "Epoch 352/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0236 - accuracy: 0.9922 - val_loss: 0.0309 - val_accuracy: 0.9912\n",
      "Epoch 353/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0240 - accuracy: 0.9918 - val_loss: 0.0353 - val_accuracy: 0.9930\n",
      "Epoch 354/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0171 - accuracy: 0.9947 - val_loss: 0.0378 - val_accuracy: 0.9912\n",
      "Epoch 355/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0201 - accuracy: 0.9926 - val_loss: 0.0333 - val_accuracy: 0.9912\n",
      "Epoch 356/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0232 - accuracy: 0.9912 - val_loss: 0.0392 - val_accuracy: 0.9877\n",
      "Epoch 357/1000\n",
      "161/161 [==============================] - 1s 9ms/step - loss: 0.0207 - accuracy: 0.9938 - val_loss: 0.0386 - val_accuracy: 0.9912\n",
      "Epoch 358/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0170 - accuracy: 0.9947 - val_loss: 0.0408 - val_accuracy: 0.9895\n",
      "Epoch 359/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0228 - accuracy: 0.9918 - val_loss: 0.0286 - val_accuracy: 0.9930\n",
      "Epoch 360/1000\n",
      "161/161 [==============================] - 1s 9ms/step - loss: 0.0218 - accuracy: 0.9920 - val_loss: 0.0287 - val_accuracy: 0.9895\n",
      "Epoch 361/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0209 - accuracy: 0.9920 - val_loss: 0.0377 - val_accuracy: 0.9877\n",
      "Epoch 362/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0174 - accuracy: 0.9934 - val_loss: 0.0361 - val_accuracy: 0.9877\n",
      "Epoch 363/1000\n",
      "161/161 [==============================] - 2s 11ms/step - loss: 0.0198 - accuracy: 0.9936 - val_loss: 0.0305 - val_accuracy: 0.9895\n",
      "Epoch 364/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0203 - accuracy: 0.9920 - val_loss: 0.0348 - val_accuracy: 0.9895\n",
      "Epoch 365/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0196 - accuracy: 0.9928 - val_loss: 0.0362 - val_accuracy: 0.9895\n",
      "Epoch 366/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0205 - accuracy: 0.9930 - val_loss: 0.0299 - val_accuracy: 0.9912\n",
      "Epoch 367/1000\n",
      "161/161 [==============================] - 2s 10ms/step - loss: 0.0179 - accuracy: 0.9928 - val_loss: 0.0368 - val_accuracy: 0.9860\n",
      "Epoch 368/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0216 - accuracy: 0.9920 - val_loss: 0.0350 - val_accuracy: 0.9877\n",
      "Epoch 369/1000\n",
      "161/161 [==============================] - 1s 9ms/step - loss: 0.0249 - accuracy: 0.9914 - val_loss: 0.0310 - val_accuracy: 0.9877\n",
      "Epoch 370/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0199 - accuracy: 0.9920 - val_loss: 0.0355 - val_accuracy: 0.9877\n",
      "Epoch 371/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0207 - accuracy: 0.9907 - val_loss: 0.0335 - val_accuracy: 0.9912\n",
      "Epoch 372/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0183 - accuracy: 0.9932 - val_loss: 0.0346 - val_accuracy: 0.9877\n",
      "Epoch 373/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0203 - accuracy: 0.9920 - val_loss: 0.0380 - val_accuracy: 0.9895\n",
      "Epoch 374/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0221 - accuracy: 0.9912 - val_loss: 0.0356 - val_accuracy: 0.9895\n",
      "Epoch 375/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0172 - accuracy: 0.9932 - val_loss: 0.0409 - val_accuracy: 0.9877\n",
      "Epoch 376/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0246 - accuracy: 0.9914 - val_loss: 0.0374 - val_accuracy: 0.9877\n",
      "Epoch 377/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0224 - accuracy: 0.9918 - val_loss: 0.0264 - val_accuracy: 0.9912\n",
      "Epoch 378/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0259 - accuracy: 0.9912 - val_loss: 0.0337 - val_accuracy: 0.9895\n",
      "Epoch 379/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0233 - accuracy: 0.9903 - val_loss: 0.0308 - val_accuracy: 0.9912\n",
      "Epoch 380/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0203 - accuracy: 0.9932 - val_loss: 0.0307 - val_accuracy: 0.9912\n",
      "Epoch 381/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0198 - accuracy: 0.9918 - val_loss: 0.0299 - val_accuracy: 0.9877\n",
      "Epoch 382/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0184 - accuracy: 0.9930 - val_loss: 0.0368 - val_accuracy: 0.9877\n",
      "Epoch 383/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0187 - accuracy: 0.9934 - val_loss: 0.0317 - val_accuracy: 0.9912\n",
      "Epoch 384/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0200 - accuracy: 0.9922 - val_loss: 0.0292 - val_accuracy: 0.9877\n",
      "Epoch 385/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0204 - accuracy: 0.9928 - val_loss: 0.0274 - val_accuracy: 0.9930\n",
      "Epoch 386/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0222 - accuracy: 0.9922 - val_loss: 0.0330 - val_accuracy: 0.9912\n",
      "Epoch 387/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0186 - accuracy: 0.9926 - val_loss: 0.0504 - val_accuracy: 0.9877\n",
      "Epoch 388/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0183 - accuracy: 0.9930 - val_loss: 0.0351 - val_accuracy: 0.9912\n",
      "Epoch 389/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0213 - accuracy: 0.9914 - val_loss: 0.0309 - val_accuracy: 0.9895\n",
      "Epoch 390/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0190 - accuracy: 0.9934 - val_loss: 0.0324 - val_accuracy: 0.9912\n",
      "Epoch 391/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0171 - accuracy: 0.9936 - val_loss: 0.0352 - val_accuracy: 0.9912\n",
      "Epoch 392/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0163 - accuracy: 0.9951 - val_loss: 0.0334 - val_accuracy: 0.9930\n",
      "Epoch 393/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0197 - accuracy: 0.9944 - val_loss: 0.0320 - val_accuracy: 0.9895\n",
      "Epoch 394/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0174 - accuracy: 0.9936 - val_loss: 0.0330 - val_accuracy: 0.9912\n",
      "Epoch 395/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0184 - accuracy: 0.9951 - val_loss: 0.0420 - val_accuracy: 0.9877\n",
      "Epoch 396/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0182 - accuracy: 0.9924 - val_loss: 0.0337 - val_accuracy: 0.9895\n",
      "Epoch 397/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0189 - accuracy: 0.9920 - val_loss: 0.0299 - val_accuracy: 0.9895\n",
      "Epoch 398/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0206 - accuracy: 0.9932 - val_loss: 0.0324 - val_accuracy: 0.9877\n",
      "Epoch 399/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0195 - accuracy: 0.9924 - val_loss: 0.0334 - val_accuracy: 0.9895\n",
      "Epoch 400/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0181 - accuracy: 0.9930 - val_loss: 0.0360 - val_accuracy: 0.9930\n",
      "Epoch 401/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0187 - accuracy: 0.9938 - val_loss: 0.0331 - val_accuracy: 0.9912\n",
      "Epoch 402/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.9934 - val_loss: 0.0360 - val_accuracy: 0.9912\n",
      "Epoch 403/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0147 - accuracy: 0.9934 - val_loss: 0.0342 - val_accuracy: 0.9895\n",
      "Epoch 404/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0177 - accuracy: 0.9936 - val_loss: 0.0378 - val_accuracy: 0.9895\n",
      "Epoch 405/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0197 - accuracy: 0.9934 - val_loss: 0.0329 - val_accuracy: 0.9877\n",
      "Epoch 406/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0202 - accuracy: 0.9916 - val_loss: 0.0312 - val_accuracy: 0.9895\n",
      "Epoch 407/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0140 - accuracy: 0.9946 - val_loss: 0.0416 - val_accuracy: 0.9895\n",
      "Epoch 408/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0183 - accuracy: 0.9928 - val_loss: 0.0361 - val_accuracy: 0.9930\n",
      "Epoch 409/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0170 - accuracy: 0.9930 - val_loss: 0.0279 - val_accuracy: 0.9877\n",
      "Epoch 410/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0174 - accuracy: 0.9932 - val_loss: 0.0364 - val_accuracy: 0.9895\n",
      "Epoch 411/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.0327 - val_accuracy: 0.9912\n",
      "Epoch 412/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0187 - accuracy: 0.9932 - val_loss: 0.0298 - val_accuracy: 0.9895\n",
      "Epoch 413/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0166 - accuracy: 0.9930 - val_loss: 0.0297 - val_accuracy: 0.9895\n",
      "Epoch 414/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0172 - accuracy: 0.9932 - val_loss: 0.0289 - val_accuracy: 0.9912\n",
      "Epoch 415/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0170 - accuracy: 0.9928 - val_loss: 0.0279 - val_accuracy: 0.9912\n",
      "Epoch 416/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0187 - accuracy: 0.9936 - val_loss: 0.0273 - val_accuracy: 0.9930\n",
      "Epoch 417/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0185 - accuracy: 0.9934 - val_loss: 0.0281 - val_accuracy: 0.9912\n",
      "Epoch 418/1000\n",
      "161/161 [==============================] - 1s 9ms/step - loss: 0.0147 - accuracy: 0.9955 - val_loss: 0.0321 - val_accuracy: 0.9912\n",
      "Epoch 419/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0204 - accuracy: 0.9928 - val_loss: 0.0285 - val_accuracy: 0.9930\n",
      "Epoch 420/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0148 - accuracy: 0.9944 - val_loss: 0.0373 - val_accuracy: 0.9860\n",
      "Epoch 421/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0164 - accuracy: 0.9951 - val_loss: 0.0298 - val_accuracy: 0.9895\n",
      "Epoch 422/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0191 - accuracy: 0.9928 - val_loss: 0.0361 - val_accuracy: 0.9877\n",
      "Epoch 423/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0214 - accuracy: 0.9916 - val_loss: 0.0310 - val_accuracy: 0.9895\n",
      "Epoch 424/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0147 - accuracy: 0.9951 - val_loss: 0.0373 - val_accuracy: 0.9895\n",
      "Epoch 425/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0181 - accuracy: 0.9934 - val_loss: 0.0307 - val_accuracy: 0.9930\n",
      "Epoch 426/1000\n",
      "161/161 [==============================] - 2s 12ms/step - loss: 0.0140 - accuracy: 0.9949 - val_loss: 0.0380 - val_accuracy: 0.9895\n",
      "Epoch 427/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0192 - accuracy: 0.9928 - val_loss: 0.0341 - val_accuracy: 0.9930\n",
      "Epoch 428/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0179 - accuracy: 0.9928 - val_loss: 0.0412 - val_accuracy: 0.9860\n",
      "Epoch 429/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0210 - accuracy: 0.9928 - val_loss: 0.0336 - val_accuracy: 0.9877\n",
      "Epoch 430/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0187 - accuracy: 0.9932 - val_loss: 0.0303 - val_accuracy: 0.9930\n",
      "Epoch 431/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0185 - accuracy: 0.9918 - val_loss: 0.0361 - val_accuracy: 0.9877\n",
      "Epoch 432/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0159 - accuracy: 0.9944 - val_loss: 0.0282 - val_accuracy: 0.9930\n",
      "Epoch 433/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0183 - accuracy: 0.9932 - val_loss: 0.0301 - val_accuracy: 0.9912\n",
      "Epoch 434/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0132 - accuracy: 0.9953 - val_loss: 0.0334 - val_accuracy: 0.9895\n",
      "Epoch 435/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0130 - accuracy: 0.9955 - val_loss: 0.0383 - val_accuracy: 0.9895\n",
      "Epoch 436/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0210 - accuracy: 0.9914 - val_loss: 0.0330 - val_accuracy: 0.9912\n",
      "Epoch 437/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0160 - accuracy: 0.9944 - val_loss: 0.0356 - val_accuracy: 0.9877\n",
      "Epoch 438/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0179 - accuracy: 0.9920 - val_loss: 0.0359 - val_accuracy: 0.9895\n",
      "Epoch 439/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0208 - accuracy: 0.9924 - val_loss: 0.0264 - val_accuracy: 0.9912\n",
      "Epoch 440/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0196 - accuracy: 0.9932 - val_loss: 0.0287 - val_accuracy: 0.9895\n",
      "Epoch 441/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0182 - accuracy: 0.9934 - val_loss: 0.0263 - val_accuracy: 0.9930\n",
      "Epoch 442/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0146 - accuracy: 0.9953 - val_loss: 0.0414 - val_accuracy: 0.9860\n",
      "Epoch 443/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0160 - accuracy: 0.9947 - val_loss: 0.0321 - val_accuracy: 0.9895\n",
      "Epoch 444/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0162 - accuracy: 0.9940 - val_loss: 0.0338 - val_accuracy: 0.9895\n",
      "Epoch 445/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0166 - accuracy: 0.9942 - val_loss: 0.0252 - val_accuracy: 0.9912\n",
      "Epoch 446/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0161 - accuracy: 0.9955 - val_loss: 0.0261 - val_accuracy: 0.9930\n",
      "Epoch 447/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0128 - accuracy: 0.9947 - val_loss: 0.0319 - val_accuracy: 0.9930\n",
      "Epoch 448/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0110 - accuracy: 0.9955 - val_loss: 0.0293 - val_accuracy: 0.9930\n",
      "Epoch 449/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0178 - accuracy: 0.9934 - val_loss: 0.0299 - val_accuracy: 0.9877\n",
      "Epoch 450/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0206 - accuracy: 0.9934 - val_loss: 0.0280 - val_accuracy: 0.9912\n",
      "Epoch 451/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0126 - accuracy: 0.9951 - val_loss: 0.0285 - val_accuracy: 0.9912\n",
      "Epoch 452/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0180 - accuracy: 0.9940 - val_loss: 0.0328 - val_accuracy: 0.9912\n",
      "Epoch 453/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0151 - accuracy: 0.9936 - val_loss: 0.0356 - val_accuracy: 0.9912\n",
      "Epoch 454/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0169 - accuracy: 0.9934 - val_loss: 0.0330 - val_accuracy: 0.9912\n",
      "Epoch 455/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0180 - accuracy: 0.9932 - val_loss: 0.0322 - val_accuracy: 0.9912\n",
      "Epoch 456/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0129 - accuracy: 0.9957 - val_loss: 0.0241 - val_accuracy: 0.9912\n"
     ]
    }
   ],
   "source": [
    "class customcallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs.get(\"loss\") < 0.025 and logs.get(\"val_loss\") < 0.025:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss= tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32,\n",
    "                    callbacks=[customcallback()],\n",
    "                    epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(X_test,y_test)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mmetrics_names)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ASUS\\AppData\\Local\\Temp\\tmptun08b1s\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ASUS\\AppData\\Local\\Temp\\tmptun08b1s\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 76KB\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "print('Model size: %dKB' % (len(tflite_model) / 1024))\n",
    "\n",
    "with open('../Trainer/model/pose_classifier.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
